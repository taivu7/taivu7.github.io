{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4gXSnKtVnpbs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV9ZhaxnoJpG",
        "outputId": "5e6e3758-144e-4273-f2ec-6b1b2917f8f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architecture"
      ],
      "metadata": {
        "id": "v_3-dFEzodXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "    assert d_model % n_heads == 0\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = d_model // n_heads\n",
        "\n",
        "    self.q_linear = nn.Linear(d_model, self.d_k * n_heads)\n",
        "    self.k_linear = nn.Linear(d_model, self.d_k * n_heads)\n",
        "    self.v_linear = nn.Linear(d_model, self.d_k * n_heads) # Because d_v = d_k\n",
        "    self.out = nn.Linear(d_model, self.d_k * n_heads)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "\n",
        "    batch_size = query.size(0)\n",
        "\n",
        "    Q = self.q_linear(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) # batch_size, n_head, seq_len, d_k\n",
        "    K = self.k_linear(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) # batch_size, n_head, seq_len, d_k\n",
        "    V = self.v_linear(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) # batch_size, n_head, seq_len, d_k\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # batch_size, n_head, seq_len, seq_len\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scores, dim = -1) # batch_size, n_head, seq_len, seq_len\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Concatenate heads and put through final linear layer\n",
        "    context = torch.matmul(attention_weights, V) # batch_size, n_head, seq_len, d_k\n",
        "\n",
        "    context = context.transpose(1, 2).contiguous().view(\n",
        "        batch_size, -1, self.d_model\n",
        "    )\n",
        "\n",
        "    output = self.out(context) # batch_size, seq_len, d_model\n",
        "\n",
        "    return output, attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "0c2yWwujoa2b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, max_len: int):\n",
        "    super().__init__()\n",
        "\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                         (-math.log(10000.0)/d_model))\n",
        "\n",
        "    pe[:,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:x.size(0), :]"
      ],
      "metadata": {
        "id": "2BGgwnxLEJAy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "_905noIP8Elp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "\n",
        "    attn_output, attention_weights = self.attention(x, x, x, mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "    ff_output = self.feed_forward(x)\n",
        "\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "KYpKV1s0henX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size: int, d_model: int, n_heads: int,\n",
        "               n_layers: int, d_ff: int, max_len: int, dropout: float = 0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "    self.transformer_blocks = nn.ModuleList([\n",
        "        TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
        "        for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "    self.ln_f = nn.LayerNorm(d_model)\n",
        "    self.head = nn.Linear(d_model, vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      torch.nn.init.zeros_(module.bias)\n",
        "      torch.nn.init.ones_(module.weight)\n",
        "\n",
        "  def create_causal_mask(self, size):\n",
        "    mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "\n",
        "    batch_size, seq_len = x.size()\n",
        "\n",
        "    mask = self.create_causal_mask(seq_len).to(x.device)\n",
        "\n",
        "    # Token embedding + positional encoding\n",
        "    x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "    x = self.pos_encoding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Pass through transformer blocks\n",
        "    attention_weights = []\n",
        "    for block in self.transformer_blocks:\n",
        "      x, atten = block(x, mask)\n",
        "      attention_weights.append(atten)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.head(x)\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "    return logits, loss, attention_weights"
      ],
      "metadata": {
        "id": "jGw9HFrU0f-s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTextDataset(Dataset):\n",
        "\n",
        "  def __init__(self, text: str, seq_len: int):\n",
        "\n",
        "    self.chars = sorted(list(set(text)))\n",
        "    self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "    self.idx_to_char = {i:ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    self.data = [self.char_to_idx[ch] for ch in text]\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.seq_len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.tensor(self.data[idx:idx + self.seq_len], dtype=torch.long)\n",
        "    y = torch.tensor(self.data[idx+1:idx+self.seq_len+1], dtype = torch.long)\n",
        "    return x, y\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    return len(self.chars)"
      ],
      "metadata": {
        "id": "SkHNSuoY3AXF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, dataset, start_text: str, max_new_tokens: int = 100,\n",
        "                  temperature: float = 1.0):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    context = [dataset.char_to_idx[ch] for ch in start_text]\n",
        "    context = torch.tensor(context, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generate = []\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      logits, _, _ = model(context)\n",
        "\n",
        "      # Focus on the last time step\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "\n",
        "      # Sample from the distribution\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      # Append to context and generated sequence\n",
        "      context = torch.cat([context, next_token], dim=1)\n",
        "      generate.append(next_token.item())\n",
        "\n",
        "      # Keep context length manageable\n",
        "      if context.size(1) > 100:\n",
        "        context = context[:, -100:]\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_text = ''.join([dataset.idx_to_char[token] for token in generate])\n",
        "\n",
        "    return start_text + generated_text\n"
      ],
      "metadata": {
        "id": "8RAlEZ9nAJ5p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "\n",
        "  sample_text = \"\"\"\n",
        "    In the beginning was the Word, and the Word was with God, and the Word was God.\n",
        "    All things were made through him, and without him was not any thing made that was made.\n",
        "    In him was life, and the life was the light of men.\n",
        "    The light shines in the darkness, and the darkness has not overcome it.\n",
        "    There was a man sent from God, whose name was John.\n",
        "    He came as a witness, to bear witness about the light, that all might believe through him.\n",
        "    He was not the light, but came to bear witness about the light.\n",
        "    The true light, which gives light to everyone, was coming into the world.\n",
        "    He was in the world, and the world was made through him, yet the world did not know him.\n",
        "    He came to his own, and his own people did not receive him.\n",
        "    But to all who did receive him, who believed in his name, he gave the right to become children of God.\n",
        "  \"\"\"\n",
        "\n",
        "  # Hyperparameters\n",
        "  seq_len = 64\n",
        "  d_model = 128\n",
        "  n_heads = 8\n",
        "  n_layers = 4\n",
        "  d_ff = 512\n",
        "  batch_size = 32\n",
        "  learning_rate = 0.001\n",
        "  num_epochs = 50\n",
        "\n",
        "  dataset = SimpleTextDataset(sample_text, seq_len)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = TinyTransformer(\n",
        "      vocab_size=dataset.vocab_size,\n",
        "      d_model = d_model,\n",
        "      n_heads = n_heads,\n",
        "      n_layers = n_layers,\n",
        "      d_ff = d_ff,\n",
        "      max_len = seq_len * 2\n",
        "  ).to(device)\n",
        "\n",
        "  # Count parameters\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f\"Total parameters: {total_params}\")\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "  # Training loop\n",
        "  losses = []\n",
        "  model.train()\n",
        "\n",
        "  print(\"\\nStarting training...\")\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logits, loss, _ = model(x, y)\n",
        "      loss.backward()\n",
        "\n",
        "      # Gradient clipping\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      num_batches += 1\n",
        "\n",
        "      if batch_idx % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    # Generate sample text every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(\"\\n\" + \"=\"*50)\n",
        "      print(\"Generated text sample: \")\n",
        "      generated = generate_text(model, dataset, \"At the end of the day\", max_new_tokens=100)\n",
        "      print(generated)\n",
        "      print(\"=\"*50 + \"\\n\")\n",
        "      model.train()\n",
        "\n",
        "  torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab_size': dataset.vocab_size,\n",
        "    'char_to_idx': dataset.char_to_idx,\n",
        "    'idx_to_char': dataset.idx_to_char,\n",
        "    'hyperparameters': {\n",
        "        'd_model': d_model,\n",
        "        'n_heads': n_heads,\n",
        "        'n_layers': n_layers,\n",
        "        'd_ff': d_ff,\n",
        "        'seq_len': seq_len\n",
        "    }\n",
        "  }, 'tiny_transformer.pth')\n",
        "\n",
        "  return model, dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N5f7d65wPwnJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, dataset = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_NXVSvJHlOg",
        "outputId": "248f5922-6de9-4b12-af70-9f483d924b38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 802082\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/50, Batch 0, Loss: 3.5463\n",
            "Epoch 1/50, Batch 10, Loss: 2.7826\n",
            "Epoch 1/50, Batch 20, Loss: 2.3042\n",
            "Epoch 2/50, Batch 0, Loss: 2.1719\n",
            "Epoch 2/50, Batch 10, Loss: 2.0383\n",
            "Epoch 2/50, Batch 20, Loss: 1.9541\n",
            "Epoch 3/50, Batch 0, Loss: 1.9228\n",
            "Epoch 3/50, Batch 10, Loss: 1.8023\n",
            "Epoch 3/50, Batch 20, Loss: 1.7883\n",
            "Epoch 4/50, Batch 0, Loss: 1.7654\n",
            "Epoch 4/50, Batch 10, Loss: 1.6823\n",
            "Epoch 4/50, Batch 20, Loss: 1.5630\n",
            "Epoch 5/50, Batch 0, Loss: 1.5140\n",
            "Epoch 5/50, Batch 10, Loss: 1.5284\n",
            "Epoch 5/50, Batch 20, Loss: 1.4052\n",
            "Epoch 6/50, Batch 0, Loss: 1.3606\n",
            "Epoch 6/50, Batch 10, Loss: 1.2273\n",
            "Epoch 6/50, Batch 20, Loss: 1.0820\n",
            "Epoch 7/50, Batch 0, Loss: 1.1037\n",
            "Epoch 7/50, Batch 10, Loss: 0.9935\n",
            "Epoch 7/50, Batch 20, Loss: 0.8738\n",
            "Epoch 8/50, Batch 0, Loss: 0.8610\n",
            "Epoch 8/50, Batch 10, Loss: 0.7918\n",
            "Epoch 8/50, Batch 20, Loss: 0.7245\n",
            "Epoch 9/50, Batch 0, Loss: 0.7086\n",
            "Epoch 9/50, Batch 10, Loss: 0.6231\n",
            "Epoch 9/50, Batch 20, Loss: 0.5964\n",
            "Epoch 10/50, Batch 0, Loss: 0.5500\n",
            "Epoch 10/50, Batch 10, Loss: 0.5628\n",
            "Epoch 10/50, Batch 20, Loss: 0.4969\n",
            "\n",
            "==================================================\n",
            "Generated text sample: \n",
            "At the end of the day de didenanas t thenye the dathe worldind was t made thpenove hig hrld knecomamim.\n",
            "     He came ame \n",
            "==================================================\n",
            "\n",
            "Epoch 11/50, Batch 0, Loss: 0.4568\n",
            "Epoch 11/50, Batch 10, Loss: 0.4723\n",
            "Epoch 11/50, Batch 20, Loss: 0.3949\n",
            "Epoch 12/50, Batch 0, Loss: 0.4055\n",
            "Epoch 12/50, Batch 10, Loss: 0.3815\n",
            "Epoch 12/50, Batch 20, Loss: 0.3647\n",
            "Epoch 13/50, Batch 0, Loss: 0.3550\n",
            "Epoch 13/50, Batch 10, Loss: 0.3213\n",
            "Epoch 13/50, Batch 20, Loss: 0.3166\n",
            "Epoch 14/50, Batch 0, Loss: 0.2959\n",
            "Epoch 14/50, Batch 10, Loss: 0.2961\n",
            "Epoch 14/50, Batch 20, Loss: 0.2801\n",
            "Epoch 15/50, Batch 0, Loss: 0.2448\n",
            "Epoch 15/50, Batch 10, Loss: 0.2660\n",
            "Epoch 15/50, Batch 20, Loss: 0.2437\n",
            "Epoch 16/50, Batch 0, Loss: 0.2448\n",
            "Epoch 16/50, Batch 10, Loss: 0.2685\n",
            "Epoch 16/50, Batch 20, Loss: 0.2682\n",
            "Epoch 17/50, Batch 0, Loss: 0.2069\n",
            "Epoch 17/50, Batch 10, Loss: 0.2303\n",
            "Epoch 17/50, Batch 20, Loss: 0.1975\n",
            "Epoch 18/50, Batch 0, Loss: 0.1800\n",
            "Epoch 18/50, Batch 10, Loss: 0.2125\n",
            "Epoch 18/50, Batch 20, Loss: 0.1826\n",
            "Epoch 19/50, Batch 0, Loss: 0.1986\n",
            "Epoch 19/50, Batch 10, Loss: 0.1839\n",
            "Epoch 19/50, Batch 20, Loss: 0.2085\n",
            "Epoch 20/50, Batch 0, Loss: 0.1765\n",
            "Epoch 20/50, Batch 10, Loss: 0.1828\n",
            "Epoch 20/50, Batch 20, Loss: 0.1939\n",
            "\n",
            "==================================================\n",
            "Generated text sample: \n",
            "At the end of the dayon.\n",
            "     In him was lin, wn, peid not receive him, who be becas ito bear as a abe witome be witouthi\n",
            "==================================================\n",
            "\n",
            "Epoch 21/50, Batch 0, Loss: 0.1827\n",
            "Epoch 21/50, Batch 10, Loss: 0.1575\n",
            "Epoch 21/50, Batch 20, Loss: 0.1911\n",
            "Epoch 22/50, Batch 0, Loss: 0.2013\n",
            "Epoch 22/50, Batch 10, Loss: 0.1897\n",
            "Epoch 22/50, Batch 20, Loss: 0.1805\n",
            "Epoch 23/50, Batch 0, Loss: 0.1673\n",
            "Epoch 23/50, Batch 10, Loss: 0.1719\n",
            "Epoch 23/50, Batch 20, Loss: 0.1709\n",
            "Epoch 24/50, Batch 0, Loss: 0.1565\n",
            "Epoch 24/50, Batch 10, Loss: 0.1555\n",
            "Epoch 24/50, Batch 20, Loss: 0.1479\n",
            "Epoch 25/50, Batch 0, Loss: 0.1566\n",
            "Epoch 25/50, Batch 10, Loss: 0.1341\n",
            "Epoch 25/50, Batch 20, Loss: 0.1472\n",
            "Epoch 26/50, Batch 0, Loss: 0.1455\n",
            "Epoch 26/50, Batch 10, Loss: 0.1560\n",
            "Epoch 26/50, Batch 20, Loss: 0.1561\n",
            "Epoch 27/50, Batch 0, Loss: 0.1468\n",
            "Epoch 27/50, Batch 10, Loss: 0.1297\n",
            "Epoch 27/50, Batch 20, Loss: 0.1447\n",
            "Epoch 28/50, Batch 0, Loss: 0.1444\n",
            "Epoch 28/50, Batch 10, Loss: 0.1428\n",
            "Epoch 28/50, Batch 20, Loss: 0.1551\n",
            "Epoch 29/50, Batch 0, Loss: 0.1381\n",
            "Epoch 29/50, Batch 10, Loss: 0.1412\n",
            "Epoch 29/50, Batch 20, Loss: 0.1470\n",
            "Epoch 30/50, Batch 0, Loss: 0.1416\n",
            "Epoch 30/50, Batch 10, Loss: 0.1339\n",
            "Epoch 30/50, Batch 20, Loss: 0.1411\n",
            "\n",
            "==================================================\n",
            "Generated text sample: \n",
            "At the end of the dayot was me.\n",
            "    He was in the world, and the world was made through him, yethout was no as abecame th\n",
            "==================================================\n",
            "\n",
            "Epoch 31/50, Batch 0, Loss: 0.1319\n",
            "Epoch 31/50, Batch 10, Loss: 0.1352\n",
            "Epoch 31/50, Batch 20, Loss: 0.1425\n",
            "Epoch 32/50, Batch 0, Loss: 0.1291\n",
            "Epoch 32/50, Batch 10, Loss: 0.1294\n",
            "Epoch 32/50, Batch 20, Loss: 0.1324\n",
            "Epoch 33/50, Batch 0, Loss: 0.1313\n",
            "Epoch 33/50, Batch 10, Loss: 0.1308\n",
            "Epoch 33/50, Batch 20, Loss: 0.1269\n",
            "Epoch 34/50, Batch 0, Loss: 0.1375\n",
            "Epoch 34/50, Batch 10, Loss: 0.1304\n",
            "Epoch 34/50, Batch 20, Loss: 0.1333\n",
            "Epoch 35/50, Batch 0, Loss: 0.1353\n",
            "Epoch 35/50, Batch 10, Loss: 0.1293\n",
            "Epoch 35/50, Batch 20, Loss: 0.1046\n",
            "Epoch 36/50, Batch 0, Loss: 0.1179\n",
            "Epoch 36/50, Batch 10, Loss: 0.1257\n",
            "Epoch 36/50, Batch 20, Loss: 0.1269\n",
            "Epoch 37/50, Batch 0, Loss: 0.1186\n",
            "Epoch 37/50, Batch 10, Loss: 0.1152\n",
            "Epoch 37/50, Batch 20, Loss: 0.1278\n",
            "Epoch 38/50, Batch 0, Loss: 0.1122\n",
            "Epoch 38/50, Batch 10, Loss: 0.1217\n",
            "Epoch 38/50, Batch 20, Loss: 0.1172\n",
            "Epoch 39/50, Batch 0, Loss: 0.1053\n",
            "Epoch 39/50, Batch 10, Loss: 0.1383\n",
            "Epoch 39/50, Batch 20, Loss: 0.1230\n",
            "Epoch 40/50, Batch 0, Loss: 0.1083\n",
            "Epoch 40/50, Batch 10, Loss: 0.1306\n",
            "Epoch 40/50, Batch 20, Loss: 0.1330\n",
            "\n",
            "==================================================\n",
            "Generated text sample: \n",
            "At the end of the dayo, was man.\n",
            "    He was in the world, and the world was made through him, yet the was not mad the.\n",
            " h\n",
            "==================================================\n",
            "\n",
            "Epoch 41/50, Batch 0, Loss: 0.1145\n",
            "Epoch 41/50, Batch 10, Loss: 0.1198\n",
            "Epoch 41/50, Batch 20, Loss: 0.1249\n",
            "Epoch 42/50, Batch 0, Loss: 0.1227\n",
            "Epoch 42/50, Batch 10, Loss: 0.1194\n",
            "Epoch 42/50, Batch 20, Loss: 0.1126\n",
            "Epoch 43/50, Batch 0, Loss: 0.1164\n",
            "Epoch 43/50, Batch 10, Loss: 0.1131\n",
            "Epoch 43/50, Batch 20, Loss: 0.1157\n",
            "Epoch 44/50, Batch 0, Loss: 0.1115\n",
            "Epoch 44/50, Batch 10, Loss: 0.1227\n",
            "Epoch 44/50, Batch 20, Loss: 0.1143\n",
            "Epoch 45/50, Batch 0, Loss: 0.1043\n",
            "Epoch 45/50, Batch 10, Loss: 0.1001\n",
            "Epoch 45/50, Batch 20, Loss: 0.1140\n",
            "Epoch 46/50, Batch 0, Loss: 0.1044\n",
            "Epoch 46/50, Batch 10, Loss: 0.1156\n",
            "Epoch 46/50, Batch 20, Loss: 0.1077\n",
            "Epoch 47/50, Batch 0, Loss: 0.1090\n",
            "Epoch 47/50, Batch 10, Loss: 0.1060\n",
            "Epoch 47/50, Batch 20, Loss: 0.1050\n",
            "Epoch 48/50, Batch 0, Loss: 0.1041\n",
            "Epoch 48/50, Batch 10, Loss: 0.1104\n",
            "Epoch 48/50, Batch 20, Loss: 0.1160\n",
            "Epoch 49/50, Batch 0, Loss: 0.1215\n",
            "Epoch 49/50, Batch 10, Loss: 0.1091\n",
            "Epoch 49/50, Batch 20, Loss: 0.1056\n",
            "Epoch 50/50, Batch 0, Loss: 0.1192\n",
            "Epoch 50/50, Batch 10, Loss: 0.1187\n",
            "Epoch 50/50, Batch 20, Loss: 0.1159\n",
            "\n",
            "==================================================\n",
            "Generated text sample: \n",
            "At the end of the dayon.\n",
            "    was in the was me wade s thorlid knes hinown, his people d receive his name he ghtherowas Go\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16Ww_L4FQUbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}